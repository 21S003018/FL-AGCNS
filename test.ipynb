{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# # path = \"data/Physics/0_uncopynode.pkl\"\n",
    "# # with open(path, 'rb') as f:\n",
    "# #     data = pickle.load(f)\n",
    "# # print(data.edge_index.max())\n",
    "# # print(len(data.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# # print(data.edge_index.shape, data.train_mask.shape)\n",
    "# # s = set()\n",
    "# # for e in data.edge_index[1]:\n",
    "# #     s.add(e.item())\n",
    "# # print(len(s))\n",
    "# # data.edge_index\n",
    "# # data.train_mask\n",
    "\n",
    "# # path = 'data/SBM/0_uncopynode.pkl'\n",
    "# # with open(path, 'rb') as f:\n",
    "# #     data = pickle.load(f)\n",
    "# # print(data.y.min(),data.y.max())\n",
    "# # print(data.edge_index[0].max())\n",
    "# # print(data.edge_index[1].max())\n",
    "# # print(len(data.x[0]))\n",
    "# import torch\n",
    "# x = torch.Tensor([0,0])\n",
    "# print(x.to('cuda').grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# class DotDict(dict):\n",
    "#     def __init__(self, **kwds):\n",
    "#         self.update(kwds)\n",
    "#         self.__dict__ = self\n",
    "\n",
    "# path_sbm_train = \"data/SBM/SBM_CLUSTER_train.pkl\"\n",
    "# path_sbm_val = \"data/SBM/SBM_CLUSTER_val.pkl\"\n",
    "# path_sbm_test = \"data/SBM/SBM_CLUSTER_test.pkl\"\n",
    "# with open(path_sbm_train, 'rb') as f:\n",
    "#     sbm_train = pickle.load(f)\n",
    "# with open(path_sbm_val, 'rb') as f:\n",
    "#     sbm_val = pickle.load(f)\n",
    "# with open(path_sbm_test, 'rb') as f:\n",
    "#     sbm_test = pickle.load(f)\n",
    "# # sbm_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for tmp in sbm_train:\n",
    "# #     print(len(tmp.node_feat))\n",
    "# # sbm_train[0].node_feat\n",
    "# # edge_index = torch.tensor([[],[]])\n",
    "# # print(edge_index)\n",
    "# # print(torch.cat((edge_index, torch.nonzero(sbm_train[0].W).T+100), 1))\n",
    "# # x = torch.BoolTensor([0]*10)\n",
    "# # x[1:3] = True\n",
    "# # print(x)\n",
    "# # print([1,2]*2)\n",
    "# torch.nonzero(sbm_train[0].node_feat)\n",
    "\n",
    "# torch.nonzero(sbm_train[0].W).T[:,torch.nonzero(sbm_train[0].W).T[0] == 20]\n",
    "# for index in [  9,  13,  15,  19,  23,  24,  26,  31,  35,  41,  51,  52,  55,  58,\n",
    "#           64,  65,  67,  72,  75,  76,  78,  80,  84,  86,  87,  89,  90,  92,\n",
    "#           93,  94,  96,  99, 101, 102, 105, 111, 115]:\n",
    "#     print(sbm_train[0].node_label[index])\n",
    "# # s_train = 0\n",
    "# # for tmp in sbm_train:\n",
    "# #     s_train += len(tmp.node_feat)\n",
    "# # s_val = 0\n",
    "# # for tmp in sbm_val:\n",
    "# #     s_val += len(tmp.node_feat)\n",
    "# # s_test = 0\n",
    "# # for tmp in sbm_test:\n",
    "# #     s_test += len(tmp.node_feat)\n",
    "# # print(s_train, s_val, s_test, s_train + s_val+s_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.data import Data\n",
    "# K=1000\n",
    "# NBTRAIN = len(sbm_train)\n",
    "# NBVAL = len(sbm_val)\n",
    "# NBTEST = len(sbm_test)\n",
    "\n",
    "# import torch\n",
    "# for k in range(K):\n",
    "#     print(k)\n",
    "#     idx_base=0\n",
    "#     edge_index = torch.LongTensor([[],[]])\n",
    "#     x = torch.tensor([])\n",
    "#     y = torch.LongTensor([])\n",
    "#     for idx_graph in range(k*int(NBTRAIN/K),(k+1)*int(NBTRAIN/K)):\n",
    "#         sub_graph = sbm_train[idx_graph]\n",
    "#         edge_index = torch.cat(\n",
    "#             (edge_index, torch.nonzero(sub_graph.W).T+idx_base), 1)\n",
    "#         x = torch.cat((x, torch.unsqueeze(sub_graph.node_feat, 1)), 0)\n",
    "#         y = torch.cat((y, sub_graph.node_label), 0)\n",
    "#         idx_base += len(sub_graph.node_feat)\n",
    "#     # print(x.size(), edge_index, idx_base)\n",
    "#     idx_train = idx_base\n",
    "#     for idx_graph in range(k*int(NBVAL/K), (k+1)*int(NBVAL/K)):\n",
    "#         sub_graph = sbm_val[idx_graph]\n",
    "#         edge_index = torch.cat(\n",
    "#             (edge_index, torch.nonzero(sub_graph.W).T+idx_base), 1)\n",
    "#         x = torch.cat((x, torch.unsqueeze(sub_graph.node_feat, 1)), 0)\n",
    "#         y = torch.cat((y, sub_graph.node_label), 0)\n",
    "#         idx_base += len(sub_graph.node_feat)\n",
    "#     # print(x.size(), edge_index, idx_base)\n",
    "#     idx_val = idx_base\n",
    "#     for idx_graph in range(k*int(NBTEST/K), (k+1)*int(NBTEST/K)):\n",
    "#         sub_graph = sbm_test[idx_graph]\n",
    "#         edge_index = torch.cat(\n",
    "#             (edge_index, torch.nonzero(sub_graph.W).T+idx_base), 1)\n",
    "#         x = torch.cat((x, torch.unsqueeze(sub_graph.node_feat, 1)), 0)\n",
    "#         y = torch.cat((y, sub_graph.node_label), 0)\n",
    "#         idx_base += len(sub_graph.node_feat)\n",
    "#     # print(x.size(), edge_index, idx_base)\n",
    "#     idx_test = idx_base\n",
    "\n",
    "#     x = torch.zeros(x.shape[0], 7).scatter(1, torch.tensor(x,dtype=torch.int64), 1)[:, 1:]\n",
    "#     train_mask = torch.BoolTensor([0]*idx_base)\n",
    "#     train_mask[0:idx_train] = True\n",
    "#     val_mask = torch.BoolTensor([0]*idx_base)\n",
    "#     val_mask[idx_train:idx_val] = True\n",
    "#     test_mask = torch.BoolTensor([0]*idx_base)\n",
    "#     test_mask[idx_val:idx_test] = True\n",
    "#     labels = y[train_mask]\n",
    "#     weight = (labels.size(0) - torch.bincount(labels)).float() / labels.size(0)\n",
    "#     with open(\"data/SBM/{}_uncopynode.pkl\".format(k), \"wb\") as f:\n",
    "#         pickle.dump(Data(edge_index=edge_index, x = x, y = y, train_mask = train_mask, val_mask = val_mask, test_mask = val_mask, weight=weight), f)\n",
    "#     # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.data import Data\n",
    "# K=1000\n",
    "# NBTRAIN = len(sbm_train)\n",
    "# NBVAL = len(sbm_val)\n",
    "# NBTEST = len(sbm_test)\n",
    "\n",
    "# import torch\n",
    "# for k in range(K):\n",
    "#     print(k)\n",
    "#     idx_base=0\n",
    "#     edge_index = torch.LongTensor([[],[]])\n",
    "#     x = torch.tensor([])\n",
    "#     y = torch.LongTensor([])\n",
    "#     for idx_graph in range(k*int(NBTRAIN/K),min((k+1)*int(NBTRAIN/K),NBTRAIN)):\n",
    "#         sub_graph = sbm_train[idx_graph]\n",
    "#         edge_index = torch.cat(\n",
    "#             (edge_index, torch.nonzero(sub_graph.W).T+idx_base), 1)\n",
    "#         x = torch.cat((x, torch.unsqueeze(sub_graph.node_feat, 1)), 0)\n",
    "#         y = torch.cat((y, sub_graph.node_label), 0)\n",
    "#         idx_base += len(sub_graph.node_feat)\n",
    "#     # print(x.size(), edge_index, idx_base)\n",
    "#     idx_train = idx_base\n",
    "#     x = torch.zeros(x.shape[0], 7).scatter(1, torch.tensor(x,dtype=torch.int64), 1)[:, 1:]\n",
    "#     with open(\"data/SBM/{}_train.pkl\".format(k), \"wb\") as f:\n",
    "#         pickle.dump(Data(edge_index=edge_index, x = x, y = y), f)\n",
    "#     # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Gat, Sage#, Gcn\n",
    "from torch import optim\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "import utils\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch_geometric.nn as gnn\n",
    "\n",
    "\n",
    "# model = Gat(6,6).to('cuda:0')\n",
    "datas = []\n",
    "for i in range(1000):\n",
    "    path = f'data/SBM/{i}_uncopynode.pkl'\n",
    "    with open(path, 'rb') as f:\n",
    "        datas.append(pickle.load(f).to('cuda:3'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True], device='cuda:3')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = datas[0].y[datas[0].train_mask]\n",
    "weight = (label.size(0) - torch.bincount(label)).float() / label.size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17478\n",
      "Iter1, loss:1.5836444767713547,train_acc:0.35456593555893157,test_acc:0.35730682588086243\n",
      "Iter2, loss:1.434618411540985,train_acc:0.44584811438165356,test_acc:0.4494415812304131\n",
      "Iter3, loss:1.3959058432579041,train_acc:0.46402776776391386,test_acc:0.46684484571630824\n",
      "Iter4, loss:1.3741841263771057,train_acc:0.47536912476556054,test_acc:0.47901100156879867\n",
      "Iter5, loss:1.3591267910003662,train_acc:0.48330062767404186,test_acc:0.4848536169076288\n",
      "Iter6, loss:1.347815717935562,train_acc:0.4889950101968066,test_acc:0.4892791744227844\n",
      "Iter7, loss:1.337858922123909,train_acc:0.4932774323673668,test_acc:0.4934152512373485\n",
      "Iter8, loss:1.3287830805778504,train_acc:0.49736935000250043,test_acc:0.49798153311120685\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38592/1336992702.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0mtest_accus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_38592/1336992702.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlnin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m# x = F.relu(self.lnout1(x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# x = F.relu(self.lnout2(x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch_geometric/nn/conv/gcn_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    189\u001b[0m                     \u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;31m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch_geometric/nn/dense/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.manual_seed(0)\n",
    "class Gcn(nn.Module):\n",
    "    def __init__(self, nfeat, nclass):\n",
    "        super(Gcn, self).__init__()\n",
    "        hdim = 64\n",
    "        self.lnin = nn.Linear(nfeat,hdim)\n",
    "        self.convs = nn.ModuleList([gnn.GCNConv(hdim, hdim) for _ in range(4)])\n",
    "        # self.lnout1 = nn.Linear(hdim, int(hdim/2), bias=True)\n",
    "        # self.lnout2 = nn.Linear(int(hdim/2), int(hdim/4), bias=True)\n",
    "        # self.lnout3 = nn.Linear(int(hdim/4), nclass, bias=True)\n",
    "        self.lnout = nn.Linear(hdim, nclass)\n",
    "        return\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lnin(x)\n",
    "        for conv in self.convs:\n",
    "            x = x+F.relu(conv(x, edge_index))\n",
    "        # x = F.relu(self.lnout1(x))\n",
    "        # x = F.relu(self.lnout2(x))\n",
    "        # x = self.lnout3(x)\n",
    "        x = self.lnout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Sage(nn.Module):\n",
    "    def __init__(self, nfeat, nclass):\n",
    "        super(Sage, self).__init__()\n",
    "        hdim = 162\n",
    "        self.lnin = nn.Linear(nfeat, hdim)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [gnn.SAGEConv(hdim, hdim) for _ in range(4)])\n",
    "        self.lnout1 = nn.Linear(hdim, int(hdim/2), bias=True)\n",
    "        self.lnout2 = nn.Linear(int(hdim/2), int(hdim/4), bias=True)\n",
    "        self.lnout3 = nn.Linear(int(hdim/4), nclass, bias=True)\n",
    "        return\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lnin(x)\n",
    "        for conv in self.convs:\n",
    "            h_in = x\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "            x = h_in + x\n",
    "        x = F.relu(self.lnout1(x))\n",
    "        x = F.relu(self.lnout2(x))\n",
    "        x = self.lnout3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Gat(nn.Module):\n",
    "    def __init__(self, nfeat, nclass):\n",
    "        super(Gat, self).__init__()\n",
    "        hdim = 162\n",
    "        self.lnin = nn.Linear(nfeat, hdim)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [gnn.GATConv(hdim, hdim) for _ in range(4)])\n",
    "        self.lnout1 = nn.Linear(hdim, int(hdim/2), bias=True)\n",
    "        self.lnout2 = nn.Linear(int(hdim/2), int(hdim/4), bias=True)\n",
    "        self.lnout3 = nn.Linear(int(hdim/4), nclass, bias=True)\n",
    "        return\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lnin(x)\n",
    "        for conv in self.convs:\n",
    "            h_in = x\n",
    "            x = F.elu(conv(x, edge_index))\n",
    "            x = h_in + x\n",
    "        x = F.relu(self.lnout1(x))\n",
    "        x = F.relu(self.lnout2(x))\n",
    "        x = self.lnout3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Sgc(nn.Module):\n",
    "    def __init__(self, nfeat, nclass):\n",
    "        super(Sgc, self).__init__()\n",
    "        hdim = 162\n",
    "        self.lnin = nn.Linear(nfeat, hdim)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [gnn.SGConv(hdim, hdim, 1) for _ in range(4)])\n",
    "        self.lnout1 = nn.Linear(hdim, int(hdim/2), bias=True)\n",
    "        self.lnout2 = nn.Linear(int(hdim/2), int(hdim/4), bias=True)\n",
    "        self.lnout3 = nn.Linear(int(hdim/4), nclass, bias=True)\n",
    "        return\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lnin(x)\n",
    "        for conv in self.convs:\n",
    "            h_in = x\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "            x = h_in + x\n",
    "        x = F.relu(self.lnout1(x))\n",
    "        x = F.relu(self.lnout2(x))\n",
    "        x = self.lnout3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Appnp(nn.Module):\n",
    "    def __init__(self, nfeat, nclass):\n",
    "        super(Appnp, self).__init__()\n",
    "        hdim = 162\n",
    "        self.lnin = nn.Linear(nfeat, hdim)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [gnn.APPNP(K=1,alpha=0.1) for _ in range(4)])\n",
    "        self.lnout1 = nn.Linear(hdim, int(hdim/2), bias=True)\n",
    "        self.lnout2 = nn.Linear(int(hdim/2), int(hdim/4), bias=True)\n",
    "        self.lnout3 = nn.Linear(int(hdim/4), nclass, bias=True)\n",
    "        return\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lnin(x)\n",
    "        for conv in self.convs:\n",
    "            h_in = x\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "            x = h_in + x\n",
    "        x = F.relu(self.lnout1(x))\n",
    "        x = F.relu(self.lnout2(x))\n",
    "        x = self.lnout3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Agnn(nn.Module):\n",
    "    def __init__(self, nfeat, nclass):\n",
    "        super(Agnn, self).__init__()\n",
    "        hdim = 162\n",
    "        self.lnin = nn.Linear(nfeat, hdim)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [gnn.AGNNConv() for _ in range(4)])\n",
    "        self.lnout1 = nn.Linear(hdim, int(hdim/2), bias=True)\n",
    "        self.lnout2 = nn.Linear(int(hdim/2), int(hdim/4), bias=True)\n",
    "        self.lnout3 = nn.Linear(int(hdim/4), nclass, bias=True)\n",
    "        return\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lnin(x)\n",
    "        for conv in self.convs:\n",
    "            h_in = x\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "            x = h_in + x\n",
    "        x = F.relu(self.lnout1(x))\n",
    "        x = F.relu(self.lnout2(x))\n",
    "        x = self.lnout3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Arma(nn.Module):\n",
    "    def __init__(self, nfeat, nclass):\n",
    "        super(Arma, self).__init__()\n",
    "        hdim = 162\n",
    "        self.lnin = nn.Linear(nfeat, hdim)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [gnn.ARMAConv(hdim, hdim, 2) for _ in range(4)])\n",
    "        self.lnout1 = nn.Linear(hdim, int(hdim/2), bias=True)\n",
    "        self.lnout2 = nn.Linear(int(hdim/2), int(hdim/4), bias=True)\n",
    "        self.lnout3 = nn.Linear(int(hdim/4), nclass, bias=True)\n",
    "        return\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lnin(x)\n",
    "        for conv in self.convs:\n",
    "            h_in = x\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "            x = h_in + x\n",
    "        x = F.relu(self.lnout1(x))\n",
    "        x = F.relu(self.lnout2(x))\n",
    "        x = self.lnout3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Gated(nn.Module):\n",
    "    def __init__(self, nfeat, nclass):\n",
    "        super(Gated, self).__init__()\n",
    "        hdim = 70\n",
    "        self.lnin = nn.Linear(nfeat, hdim)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [gnn.GatedGraphConv(hdim, 1) for _ in range(4)])\n",
    "        self.lnout1 = nn.Linear(hdim, int(hdim/2), bias=True)\n",
    "        self.lnout2 = nn.Linear(int(hdim/2), int(hdim/4), bias=True)\n",
    "        self.lnout3 = nn.Linear(int(hdim/4), nclass, bias=True)\n",
    "        return\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lnin(x)\n",
    "        for conv in self.convs:\n",
    "            h_in = x\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "            x = h_in + x\n",
    "        x = F.relu(self.lnout1(x))\n",
    "        x = F.relu(self.lnout2(x))\n",
    "        x = self.lnout3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Gcn(6, 6).to('cuda:3')\n",
    "model.train()\n",
    "print(utils.num_params(model))\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5)\n",
    "for epoch in range(1000):\n",
    "    losses = 0\n",
    "    accus = 0\n",
    "    test_accus = 0\n",
    "    for iter, data in enumerate(datas):\n",
    "        preds = model(data.x, data.edge_index)\n",
    "        labels = data.y[data.train_mask]\n",
    "        loss = F.cross_entropy(preds[data.train_mask], data.y[data.train_mask], weight=data.weight)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        accus += utils.accuracy(preds[data.train_mask],data.y[data.train_mask])\n",
    "        test_accus += utils.accuracy(preds[data.test_mask],data.y[data.test_mask])\n",
    "    print('Iter{}, loss:{},train_acc:{},test_acc:{}'.format(\n",
    "        epoch+1, losses/(iter+1), accus/(iter+1),test_accus/(iter+1)))\n",
    "    scheduler.step(losses/(iter+1))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e27ccc4603250ab3abeb046d8ea1d0ecb320aa3746dcee4c0646124a7f5fa178"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
